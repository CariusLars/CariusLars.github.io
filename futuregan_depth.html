<!DOCTYPE html>
<!--[if IE 8 ]><html class="no-js oldie ie8" lang="en"> <![endif]-->
<!--[if IE 9 ]><html class="no-js oldie ie9" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html class="no-js" lang="en"> <!--<![endif]-->
<head>

   <!--- basic page needs
   ================================================== -->
   <meta charset="utf-8">
	<title>RGB-D Video Prediction</title>
	<meta name="description" content="">  
	<meta name="author" content="">

   <!-- mobile specific metas
   ================================================== -->
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

 	<!-- CSS
   ================================================== -->
   <link rel="stylesheet" href="css/base.css">
   <link rel="stylesheet" href="css/vendor.css">  
   <link rel="stylesheet" href="css/main.css">
        

   <!-- script
   ================================================== -->
	<script src="js/modernizr.js"></script>
	<script src="js/pace.min.js"></script>

   <!-- favicons
	================================================== -->
	<link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
	<link rel="icon" href="favicon.ico" type="image/x-icon">

</head>

<body id="top">

	<!-- header 
   ================================================== -->
   <header class="short-header">   

   	<div class="gradient-block"></div>	

   	<div class="row header-content">

   		<div class="logo">
	         <a href="index.html">Author</a>
	      </div>

	   	<nav id="main-nav-wrap">
				<ul class="main-navigation sf-menu">
					<li><a href="index.html" title="">Home</a></li>									
					<li class="has-children">
						Categories
						<ul class="sub-menu">
			            <li><a href="category_machinelearning.html">Machine Learning</a></li><li><a href="category_microcontroller.html">Microcontroller</a></li>
			            <li><a href="category_hardware.html">Hardware</a></li>
			         </ul>
					</li>
					<li class="has-children current">
						Blog
						<ul class="sub-menu">
			            <!-- <li><a href="single-video.html">Video Post</a></li>
			            <li><a href="single-audio.html">Audio Post</a></li>
			            <li><a href="single-gallery.html">Gallery Post</a></li>
			            <li><a href="single-standard.html">Standard Post</a></li>-->
						<li><a href="led_sign.html">LED Sign</a></li>
						<li><a href="futuregan_depth.html">RGB-D Video GAN</a></li>
						<li><a href="LSVG.html">Latent Space Video Generation</a></li>
			         </ul>
					</li>
					<!--<li><a href="style-guide.html" title="">Styles</a></li>-->
					<li><a href="about.html" title="">About Me</a></li>
					<li><a href="contact.html" title="">Contact</a></li>										
				</ul>
			</nav> <!-- end main-nav-wrap -->

			<!--<div class="search-wrap">

				<form role="search" method="get" class="search-form" action="#">
					<label>
						<span class="hide-content">Search for:</span>
						<input type="search" class="search-field" placeholder="Type Your Keywords" value="" name="s" title="Search for:" autocomplete="off">
					</label>
					<input type="submit" class="search-submit" value="Search">
				</form>

				<a href="#" id="close-search" class="close-btn">Close</a>

			</div>--> <!-- end search wrap -->

			<div class="triggers">
				<!--<a class="search-trigger" href="#"><i class="fa fa-search"></i></a>-->
				<a class="menu-toggle" href="#"><span>Menu</span></a>
			</div> <!-- end triggers -->
   		
   	</div>     		
   	
   </header> <!-- end header -->
   

   <!-- content
   ================================================== -->
   <section id="content-wrap" class="blog-single">
   	<div class="row">
   		<div class="col-twelve">

   			<article class="format-standard">  

   				<div class="content-media">
						<div class="post-thumb">
							<img src="images/FutureGAN/title.png" style="max-width:100%;" >
						</div>  
					</div>

					<div class="primary-content">

						<h1 class="page-title">Studying RGB-D GAN Video Generation</h1>

						<ul class="entry-meta">
							<li class="date">February 04, 2020</li>
							<li class="cat"><a href="category_machinelearning.html">Machine Learning</a></li>
						</ul>						

						<p class="lead">Video Prediction using Generative Adversarial Networks is a hot topic in current research. We expanded an
						existing approach to predict dense depth maps alongside the RGB video frames and studied the performance under a series of different conditions</p>

						<p> There is a lively discussion about how to best generate video clips using Generative Adversarial Networks (GANs). One of the hot questions is how much
						physical modeling a system needs to generate realistic videos. While some approaches combine neural networks with physical models of camera motion and ray-tracing,
						others separate fore- and background into two different learning pipelines. In our project, we chose to work with FutureGAN, an approach free of any physical models and
						and solely based on deep learning. You can read the initial publication <a href="https://arxiv.org/abs/1810.01325">here</a>.</p>
						<p>While predicting future video sequences from a short input clip certainly is very fascinating and has many use-cases, the classic videos we know lack one important aspect: proportions.
						Sure, we can detect objects in a video, but we can never tell how far an object is away if we don't know it's size. If object A is twice as big but twice as far away as object B, they appear exactly
						the same in a video frame. This is why we researched the ability of FutureGAN to not only predict video frames, but also dense depth maps. You can imagine a dense depth map as an image in which the color
						of each pixel depends on how far the object at that location is away from the camera. Robots, for example, can use this additional information to build a 3D-map of their environment and place the objects
						they detect in the video into perspective.</p>
						<p>Before we look at incorporating depth predictions, let's have a look how standard video prediction works in FutureGAN. </p>
						<figure>
							<img src="images/FutureGAN/FutureGAN.png" style="width:100%">
						</figure>
						<p>The scheme above shows the high-level principle of FutureGAN. Essentially, the network takes in six consecutive frames of a video and predicts the following six frames. The images are input as a stacked sequence, resulting in a block
						of six images times 3 channels (RGB). The first part of the network is a classic fully-convolutional encoder. In less technical terms, it applies a range of filters to the input to condense the information contained in our input video sequence
						down to a low-dimensional representation (the latent space representation).<br>
						The second component of FutureGAN is, as the name suggests, a GAN. From the latent representation of our input frames, it generates six RGB images. When well trained, these images can be concatenated into a continous video clip together with the input frames.<br>
						For an easy-to-understand explanation of latent space and GANs, have a look at <a href="https://cariuslars.github.io/LSVG.html">my blog post about Latent Space Video Generation</a>.</p>
						<p>As you can probably imagine, predicting accurate videos is a very hard task. GANs have a high potential for difficult tasks, but they are notoriously hard to train. To achieve more stable learning and thus generate better videos,
						FutureGAN leverages a training technique known as Progressive Growing (PG).</p>
						<figure>
							<img src="images/FutureGAN/FutureGAN_Framework.png" style="width:100%">
							<figcaption>Image taken from <a href="https://github.com/TUM-LMF/FutureGAN">here</a></figcaption>
						</figure>
						<p>I know, that schema is a bit overwhelming at first. Let's start with the bottom row. In the row itself, you can nicely recognize the classic GAN architecture. There's a generator to generate fake samples and a discriminator that
						alternately receives fake and real samples and tries to assign each of them with a score representing their realness. The only difference is the structure of our Generator: To condition our generated videos on the input sequence,
						it has an encoder-decoder structure, just as described in the previous section. <br>
						Now let's get to the Progressive Growing. Imagine you had to describe a picture to a friend. You'd probably start with the high level structure, like "there's a person and a red car". You'd then go on and on until you reach very fine-grained
						details like "there's a leaf on the left front mirror of the car". The basic idea of Progressive Growing is very similar: We want to learn the very basic and high-level features of out input videos first, so we downsample them to 4 by 4 pixels
						resolution. That's next level basic, we don't even start with "there's a red car", we start with "there's red. And a bit of black, that's it". We simply shrink out encoder-decoder generator and only use the middle layers which receive 4x4px input and
						also predict 4x4px output. We shrink out discriminator in the same way and also downsample the real sample it gets as input.<br>
						After we've trained like this for a while and are able to predict near-perfect 4 by 4 videos that the discriminator can't distinguish from real ones, we double our resolution to 8x8. In order to not loose the high-level features we learned in the previous step,
						we keep the trained "middle layers" and just append additional convolutional layers to the generator (on both sides) and to the discriminator (on the left side). FutureGAN also uses a method to slowly fade in those layers using a ResNet-structure,
						but that is not really important for you to understand right now. We keep growing our network whenever the previous resolutions training hits a stable level until we reach our goal video resolution. The network has then learned features on all abstraction levels
						which leads to a lot better performance and more stable learning than starting at full resolution from the beginning. <br>
						If you're interested in learning more about Progressive Growing, I can highly recommend reading <a href="https://arxiv.org/abs/1710.10196">this paper</a>, they show very impressive results.
						</p>
						<p>This has been a lot of theory to take in, so let's look at some visualizations to sum up what we've learned up until here.</p>
						<figure>
							<img src="images/FutureGAN/loss.png" style="width:100%">
						</figure>
						<p>This curve is called the discriminator loss. It shows the scores which the discriminator gives to the generated videos over training time. Very abstractly speaking, a score close to 0 means the generator has come up with a realistic video,
						large negative scores means the discriminator could easily tell that the video clip was fake. <br>
						You can nicely see the overall trend of the learning process here. Every time we increase the video resolution, the scores decrease as the network has no idea about the new features yet. It then slowly learns them and manages to produce better and better
						videos until the performance stabilizes and we again grow our architecture.</p>
						<p>If you feed the network with the same sample every so many steps, you can even "watch" it learning:</p>
						<figure>
							<img src="images/FutureGAN/onesample.gif" style="width:100%">
						</figure>
						<p> What you see is a unrolled video clip. The first six images are fed into FutureGAN as input, the last six images are predicted by the network. You can see how in all resolutions, the predictions continuously get better and the whole sequence
						turns into a consistent video clip. The slight changes you see in the input frames are due to random noise which is added to facilitate a more stable learning and prevent overfitting (nothing you have to worry about to understand this blog post).<br>
						At a resolution of 128 by 128 pixels, the generated video looks like this:
						</p>
						<figure>
							<img src="images/FutureGAN/overfit.gif" style="width:40%">
						</figure>
						<p>Here, the frames with a black border are input, the ones with a red border are predicted. Pretty impressive, right?<br>
						Now we can finally get to the core of our research project. Remember that at the beginning of this blog post, I told you that so-called depth maps are very useful to have available alongside your color video frames, especially for navigation tasks.
						The FutureGAN architecture makes it very easy for us to incorporate depth predictions as the in- and output are a stack of individual image channels anyway. We simply adapt the filters in the convolutional layers to deal with additional input or output
						channels. This means we do not give the network any information on how depth works or how it is related to any of the other image channels, we simply throw it into the mix and hope that the network figures it out by itself. <br>
						In order to treat the depth channel as just another image channel, we need to normalize it. This is necessary because all our color values lie between -1 and 1, so if the depth channel suddenly reports a value of 10000, that would
						immediately send the learning process into failure (if you want to learn why, read up on a phenomenon called exploding gradients). In order to normalize the depth between -1 and 1, we need to know the maximum and minimum value of all
						depth measurements that could ever appear. The minimum is quite easy, it's 0. For the maximum, we simply choose the maximum depth the camera from our dataset can detect, which is 10 meters.</p>
						<p>Now, how do you properly research whether adding something to an existing architecture improves the results? You have to conduct an ablation study. In this context, an ablation study means that you have to try out all combinations of your idea
						in order to scientifically analyze its effect. In our case, this means that we have to built the depth maps into FutureGAN at different positions. The four possibilities are:
						<ul>
							<li>Input RGB video, predict RGB video</li>
							<li>Input RGB-D (RGB and Depth) video, predict RGB video</li>
							<li>Input RGB video, predict RGB-D video</li>
							<li>Input RGB-D video, predict RGB-D video</li>
						</ul> </p>
						<figure>
							<img src="images/FutureGAN/FutureGAN_scores.svg" style="width:100%">
						</figure>
						<p>You can see the results of our study in the table above.</p>

						<table class="tg">
							<tr>
								<th class="tg-wp8o">Ground Truth</th>
								<th class="tg-wp8o">RGB → RGB</th>
								<th class="tg-wp8o">RGB-D → RGB</th>
								<th class="tg-wp8o">RGB → RGB-D</th>
								<th class="tg-wp8o">RGB-D → RGB-D</th>
							</tr>
							<tr>
								<td class="tg-wp8o" align="center"><img src="images/FutureGAN/ground_truth.gif" style="width:40px"></td>
								<td class="tg-wp8o"><img src="images/FutureGAN/no_depth.gif" style="width:40px"></td>
								<td class="tg-wp8o"><img src="images/FutureGAN/depth_in.gif" style="width:40px"></td>
								<td class="tg-wp8o"><img src="images/FutureGAN/depth_out.gif" style="width:40px"></td>
								<td class="tg-wp8o"><img src="images/FutureGAN/depth.gif" style="width:40px"></td>
							</tr>
							<tr>
								<td class="tg-wp8o"><img src="images/FutureGAN/ground_truth_depth.gif" style="width:40px"></td>
								<td class="tg-wp8o"></td>
								<td class="tg-wp8o"></td>
								<td class="tg-wp8o"><img src="images/FutureGAN/depth_out_depth.gif" style="width:40px"></td>
								<td class="tg-wp8o"><img src="images/FutureGAN/depth_depth.gif" style="width:40px"></td>
							</tr>
						</table>

						<figure>
							<img src="images/FutureGAN/futuregan.png" style="width:100%">
						</figure>

						<p>CLOSING REMARKS HERE. If you have any questions, drop me a message! For technical details of our implementation, check out my <a href="https://github.com/CariusLars/ImprovingVideoGeneration">GitHub Repo</a>. Thanks for reading!</p>
						<!--<h2>Large Heading</h2>

						<p>Harum quidem rerum facilis est et expedita distinctio. Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus <a href="http://#">omnis voluptas assumenda est</a> id quod maxime placeat facere possimus, omnis dolor repellendus. Temporibus autem quibusdam et aut officiis debitis aut rerum necessitatibus saepe eveniet ut et.</p>

						<blockquote><p>This is a simple example of a styled blockquote. A blockquote tag typically specifies a section that is quoted from another source of some sort, or highlighting text in your post.</p></blockquote>

						<p>Odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti dolores et quas molestias excepturi sint occaecati cupiditate non provident, similique sunt in culpa. Aenean eu leo quam. Pellentesque ornare sem lacinia quam venenatis vestibulum. Nulla vitae elit libero, a pharetra augue laboris in sit minim cupidatat ut dolor voluptate enim veniam consequat occaecat fugiat in adipisicing in amet Ut nulla nisi non ut enim aliqua laborum mollit quis nostrud sed sed..</p>

						<h3>Smaller Heading</h3>

						<p>Quidem rerum facilis est et expedita distinctio. Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus, omnis voluptas assumenda est, omnis dolor repellendus.</p>

						<p>Quidem rerum facilis est et expedita distinctio. Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod maxime placeat facere possimus, omnis voluptas assumenda est, omnis dolor repellendus.</p>

<pre><code>
code {
   font-size: 1.4rem;
   margin: 0 .2rem;
   padding: .2rem .6rem;
   white-space: nowrap;
   background: #F1F1F1;
   border: 1px solid #E1E1E1;	
   border-radius: 3px;
}
</code></pre>

						<p>Odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti dolores et quas molestias excepturi sint occaecati cupiditate non provident, similique sunt in culpa.</p>

						<ul>
							<li>Donec nulla non metus auctor fringilla.
								<ul>
									<li>Lorem ipsum dolor sit amet.</li>
									<li>Lorem ipsum dolor sit amet.</li>
									<li>Lorem ipsum dolor sit amet.</li>
								</ul>
							</li>
							<li>Donec nulla non metus auctor fringilla.</li>
							<li>Donec nulla non metus auctor fringilla.</li>
						</ul>

						<p>Odio dignissimos ducimus qui blanditiis praesentium voluptatum deleniti atque corrupti dolores et quas molestias excepturi sint occaecati cupiditate non provident, similique sunt in culpa. Aenean eu leo quam. Pellentesque ornare sem lacinia quam venenatis vestibulum. Nulla vitae elit libero, a pharetra augue laboris in sit minim cupidatat ut dolor voluptate enim veniam consequat occaecat fugiat in adipisicing in amet Ut nulla nisi non ut enim aliqua laborum mollit quis nostrud sed sed..</p>
-->
						<!--<p class="tags">
		  			     	<span>Tagged in :</span>
		  				  	<a href="#">orci</a><a href="#">lectus</a><a href="#">varius</a><a href="#">turpis</a>
		  			   </p>-->

		  			   <div class="author-profile">
		  			   	<img src="images/avatars/lars_speaker.jpg" alt="">

		  			   	<div class="about">
		  			   		<h4><a href="#">Lars Carius</a></h4>
		  			   	
		  			   		<p>Master student of "Robotics, Cognition, Intelligence" at TU Munich. Entrepreneur with passion for computer vision, augmented reality, machine learning and simulation.
		  			   		</p>

		  			   		<ul class="author-social">
		  			   			<li><a href="http://www.linkedin.com/in/lars-carius">LinkedIn</a></li>
		  			   		</ul>
		  			   	</div>

					</div> <!-- end entry-primary -->		  			   

	  			   <!--<div class="pagenav group">
		  			   <div class="prev-nav">
		  			   	<a href="#" rel="prev">
		  			   		<span>Previous</span>
		  			   		Tips on Minimalist Design
		  			   	</a>
		  			   </div>
		  				<div class="next-nav">
		  					<a href="#" rel="next">
		  						<span>Next</span>
		  			   		Less Is More
		  					</a>
		  				</div>
	  				</div>-->

				</article>
   		

			</div> <!-- end col-twelve -->
   	</div> <!-- end row -->


   </section> <!-- end content -->


   <!-- footer
   ================================================== -->
   <footer>

   	<div class="footer-main">

   		<div class="row">  

	      	<div class="col-four tab-full mob-full footer-info">            

	            <h4>About This Site</h4>

	               <p>
		          	Welcome to my personal website. You'll find projects from various fields that I conducted at Uni, during hackathons or in my spare time. Have a read through my blogposts and feel free to contact me if you have any questions or are interested in further information and source code.
		          	</p>

		      </div> <!-- end footer-info -->

	      	<div class="col-two tab-1-3 mob-1-2 site-links">

	      		<h4>Site Links</h4>

	      		<ul>
					<li><a href="index.html">Home</a></li>
					<li><a href="about.html">About Me</a></li>
					<li><a href="contact.html">Contact</a></li>
					<li><a href="privacy_policy.html">Privacy Policy</a></li>
				</ul>

	      	</div> <!-- end site-links -->  

	      	<div class="col-two tab-1-3 mob-1-2 social-links">

				<h4>Social</h4>

				<ul><li><a href="http://www.linkedin.com/in/lars-carius">LinkedIn</a></li></ul>
				<ul><li><a href="https://github.com/CariusLars">GitHub</a></li></ul>
				<ul><li><a href="https://www.researchgate.net/profile/Lars_Carius">ResearchGate</a></li></ul>

			</div> <!-- end social links -->

	      	<!--<div class="col-four tab-1-3 mob-full footer-subscribe">

	      		<h4>Subscribe</h4>

	      		<p>Keep yourself updated. Subscribe to our newsletter.</p>

	      		<div class="subscribe-form">

	      			<form id="mc-form" class="group" novalidate="true">

							<input type="email" value="" name="dEmail" class="email" id="mc-email" placeholder="Type &amp; press enter" required="">

			   			<input type="submit" name="subscribe" >

		   				<label for="mc-email" class="subscribe-message"></label>

						</form>

	      		</div>

	      	</div>--> <!-- end subscribe -->

	      </div> <!-- end row -->

   	</div> <!-- end footer-main -->

      <div class="footer-bottom">
      	<div class="row">

      		<div class="col-twelve">
	      		<div class="copyright">
		         	<span>© Copyright Lars Carius 2020</span>
		         	<span>Design by <a href="http://www.styleshout.com/">styleshout</a></span>		         	
		         </div>

		         <div id="go-top">
		            <a class="smoothscroll" title="Back to Top" href="#top"><i class="icon icon-arrow-up"></i></a>
		         </div>         
	      	</div>

      	</div> 
      </div> <!-- end footer-bottom -->  

   </footer>  

   <div id="preloader"> 
    	<div id="loader"></div>
   </div> 

   <!-- Java Script
   ================================================== --> 
   <script src="js/jquery-2.1.3.min.js"></script>
   <script src="js/plugins.js"></script>
   <script src="js/main.js"></script>

</body>

</html>